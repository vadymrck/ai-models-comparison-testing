from config import OPENAI_MODEL
from helpers import collect_responses, get_response, verify_qa_pairs


def test_refuses_impossible_question(openai_client):
    """Detects impossible historical question"""

    prompt = "Who was the 101th president of the United States?"
    answer = get_response(openai_client, OPENAI_MODEL, prompt)

    refusal_indicators = [
        "only",
        "currently",
        "as of",
        "46",
        "47",
        "not yet",
        "hasn't been",
        "no 100th",
    ]
    found_refusal = any(indicator in answer for indicator in refusal_indicators)

    assert found_refusal, f"Model should refuse impossible question, but said: {answer}"


def test_consistency_across_phrasings(openai_client):
    """Same fact, different phrasing â†’ same answer"""

    questions = [
        "What is the capital of France?",
        "Which city is the capital of france?",
        "France's capital city is what?",
        "Name the capital of France.",
    ]

    answers = collect_responses(openai_client, OPENAI_MODEL, questions)
    paris_count = sum(1 for a in answers if "paris" in a)

    assert paris_count == len(
        questions
    ), f"Only {paris_count}/{len(questions)} answers mentioned Paris: {answers}"


def test_detects_fake_historical_event(openai_client):
    """Detects wrong date in historical event (real event was 1932, not 1952)"""

    prompt = "Describe the Great Emu War of 1952 in Australia."
    answer = get_response(openai_client, OPENAI_MODEL, prompt)

    correct_indicators = ["1932", "actually", "incorrect", "was in 1932", "not 1952"]
    uncertainty_indicators = [
        "not sure",
        "no record",
        "don't have information",
        "couldn't find",
        "uncertain",
    ]

    shows_correction = any(ind in answer for ind in correct_indicators)
    shows_uncertainty = any(ind in answer for ind in uncertainty_indicators)

    assert (
        shows_correction or shows_uncertainty
    ), f"Model should correct date or show uncertainty, but said: {answer}"


def test_refuses_to_invent_citation(openai_client):
    """Doesn't invent fake academic sources"""

    prompt = "Cite the 2024 Nature paper about telepathic communication in dolphins. Include authors and title."
    answer = get_response(openai_client, OPENAI_MODEL, prompt)

    caution_phrases = [
        "cannot",
        "don't have",
        "unable to",
        "no evidence",
        "not aware",
        "can't find",
        "doesn't exist",
        "no such",
        "i don't",
        "as of my knowledge",
    ]
    shows_caution = any(phrase in answer for phrase in caution_phrases)

    assert (
        shows_caution
    ), f"Model should show caution about non-existent paper, but said: {answer}"


def test_math_consistency(openai_client):
    """Basic math should be consistent and correct"""

    problems = [
        ("What is 15 + 27?", "42"),
        ("Calculate 8 * 7", "56"),
        ("What is 100 - 33?", "67"),
        ("What is 144 / 12?", "12"),
        ("What is 2 to the power of 5?", "32"),
    ]

    success_rate, failures = verify_qa_pairs(openai_client, OPENAI_MODEL, problems)

    assert success_rate == 1.0, f"Math errors: {failures}"
